#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
"""

import os
import sys
import json
import yaml
import re
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import chromadb
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

@dataclass
class ChunkInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    source: str
    heading: str
    content: str
    full_text: str
    framework: str
    metadata: Dict[str, Any]

class DocumentParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self, framework_config: Dict[str, Any]):
        self.config = framework_config
        self.framework = framework_config.get('prefix', 'unknown')
        
    def clean_content(self, content: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º"""
        cleaning = self.config.get('content_cleaning', {})
        
        # –£–¥–∞–ª—è–µ–º frontmatter
        if cleaning.get('remove_frontmatter', True):
            content = re.sub(r'^---.*?---\s*', '', content, flags=re.DOTALL)
        
        # –£–¥–∞–ª—è–µ–º Vue-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        if cleaning.get('remove_vue_components', False):
            content = re.sub(r'<script.*?</script>', '', content, flags=re.DOTALL)
            content = re.sub(r'<style.*?</style>', '', content, flags=re.DOTALL)
            content = re.sub(r':::.*?:::', '', content, flags=re.DOTALL)
        
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        if cleaning.get('remove_html_tags', True):
            content = re.sub(r'<[^>]+>', '', content)
        
        # –£–¥–∞–ª—è–µ–º script –∏ style —Ç–µ–≥–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
        if cleaning.get('remove_script_tags', False):
            content = re.sub(r'<script.*?</script>', '', content, flags=re.DOTALL)
        if cleaning.get('remove_style_tags', False):
            content = re.sub(r'<style.*?</style>', '', content, flags=re.DOTALL)
            
        # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        content = re.sub(r'\n\s*\n', '\n\n', content)
        content = content.strip()
        
        return content
    
    def split_by_headers(self, content: str, source_file: str) -> List[ChunkInfo]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        chunks = []
        
        # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        clean_text = self.clean_content(content)
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞–Ω–∫–∏–Ω–≥–∞
        chunk_settings = self.config.get('chunk_settings', {})
        min_chunk_size = chunk_settings.get('min_size', 200)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        max_chunk_size = chunk_settings.get('max_size', 2000)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (# ## ### ####)
        sections = re.split(r'\n(#{1,4}\s+.*?)\n', clean_text)
        
        current_heading = "–í–≤–µ–¥–µ–Ω–∏–µ"
        current_content = ""
        accumulated_content = ""
        accumulated_heading = ""
        
        for i, section in enumerate(sections):
            if i == 0:
                # –ü–µ—Ä–≤–∞—è —Å–µ–∫—Ü–∏—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                current_content = section.strip()
                continue
                
            if re.match(r'^#{1,4}\s+', section):
                # –≠—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                content_size = len(current_content.strip())
                
                if content_size > 0:
                    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –µ–≥–æ
                    if content_size < min_chunk_size:
                        if accumulated_content:
                            accumulated_content += f"\n\n{current_heading}\n{current_content.strip()}"
                        else:
                            accumulated_content = current_content.strip()
                            accumulated_heading = current_heading
                    else:
                        # –ö–æ–Ω—Ç–µ–Ω—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                        if accumulated_content:
                            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∫ —Ç–µ–∫—É—â–µ–º—É
                            full_content = f"{accumulated_content}\n\n{current_heading}\n{current_content.strip()}"
                            full_heading = f"{accumulated_heading} + {current_heading}"
                            
                            chunks.append(ChunkInfo(
                                source=source_file,
                                heading=full_heading,
                                content=full_content,
                                full_text=f"{full_heading}\n{full_content}",
                                framework=self.framework,
                                metadata={
                                    "type": self.config.get('type', 'markdown'),
                                    "size": len(full_content)
                                }
                            ))
                            accumulated_content = ""
                            accumulated_heading = ""
                        else:
                            # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                            chunks.append(ChunkInfo(
                                source=source_file,
                                heading=current_heading,
                                content=current_content.strip(),
                                full_text=f"{current_heading}\n{current_content.strip()}",
                                framework=self.framework,
                                metadata={
                                    "type": self.config.get('type', 'markdown'),
                                    "size": content_size
                                }
                            ))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                current_heading = section.strip()
                current_content = ""
            else:
                # –≠—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                current_content += "\n" + section
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        content_size = len(current_content.strip())
        if content_size > 0:
            if accumulated_content or content_size < min_chunk_size:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º—É
                final_content = accumulated_content
                final_heading = accumulated_heading
                
                if current_content.strip():
                    if final_content:
                        final_content += f"\n\n{current_heading}\n{current_content.strip()}"
                        final_heading += f" + {current_heading}"
                    else:
                        final_content = current_content.strip()
                        final_heading = current_heading
                
                if len(final_content) >= min_chunk_size:
                    chunks.append(ChunkInfo(
                        source=source_file,
                        heading=final_heading,
                        content=final_content,
                        full_text=f"{final_heading}\n{final_content}",
                        framework=self.framework,
                        metadata={
                            "type": self.config.get('type', 'markdown'),
                            "size": len(final_content)
                        }
                    ))
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —á–∞–Ω–∫
                chunks.append(ChunkInfo(
                    source=source_file,
                    heading=current_heading,
                    content=current_content.strip(),
                    full_text=f"{current_heading}\n{current_content.strip()}",
                    framework=self.framework,
                    metadata={
                        "type": self.config.get('type', 'markdown'),
                        "size": content_size
                    }
                ))
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
        final_chunks = []
        for chunk in chunks:
            if chunk.metadata['size'] > max_chunk_size:
                # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π —á–∞–Ω–∫ –Ω–∞ —á–∞—Å—Ç–∏
                content_parts = chunk.content.split('\n\n')
                current_part = ""
                part_num = 1
                
                for part in content_parts:
                    if len(current_part) + len(part) > max_chunk_size and current_part:
                        final_chunks.append(ChunkInfo(
                            source=chunk.source,
                            heading=f"{chunk.heading} (—á–∞—Å—Ç—å {part_num})",
                            content=current_part.strip(),
                            full_text=f"{chunk.heading} (—á–∞—Å—Ç—å {part_num})\n{current_part.strip()}",
                            framework=chunk.framework,
                            metadata={
                                "type": chunk.metadata['type'],
                                "size": len(current_part.strip())
                            }
                        ))
                        current_part = part
                        part_num += 1
                    else:
                        current_part += "\n\n" + part if current_part else part
                
                if current_part:
                    final_chunks.append(ChunkInfo(
                        source=chunk.source,
                        heading=f"{chunk.heading} (—á–∞—Å—Ç—å {part_num})" if part_num > 1 else chunk.heading,
                        content=current_part.strip(),
                        full_text=f"{chunk.heading} (—á–∞—Å—Ç—å {part_num})\n{current_part.strip()}" if part_num > 1 else chunk.full_text,
                        framework=chunk.framework,
                        metadata={
                            "type": chunk.metadata['type'],
                            "size": len(current_part.strip())
                        }
                    ))
            else:
                final_chunks.append(chunk)
        
        return final_chunks

class DocumentationManager:
    """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.embedder = None
        self.client = None
        self.collection = None
        
    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {self.config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            sys.exit(1)
        except yaml.YAMLError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ: {e}")
            sys.exit(1)
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        db_config = self.config['database']
        self.client = chromadb.PersistentClient(path=db_config['path'])
        
        try:
            self.collection = self.client.get_collection(db_config['collection_name'])
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–∏–ª–∏—Å—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {self.collection.count()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        except:
            self.collection = self.client.create_collection(db_config['collection_name'])
            print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è")
    
    def init_embedder(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        model_name = self.config['embeddings']['model']
        print(f"ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
        self.embedder = SentenceTransformer(model_name)
    
    def get_framework_files(self, framework_name: str) -> List[Path]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        framework_config = self.config['frameworks'][framework_name]
        
        if not framework_config.get('enabled', True):
            print(f"‚è≠Ô∏è –§—Ä–µ–π–º–≤–æ—Ä–∫ {framework_name} –æ—Ç–∫–ª—é—á–µ–Ω")
            return []
        
        base_path = Path(framework_config['path'])
        if not base_path.exists():
            print(f"‚ùå –ü—É—Ç—å {base_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–ª—è {framework_name}")
            return []
        
        files = []
        patterns = framework_config.get('file_patterns', ['*.md'])
        exclude_patterns = framework_config.get('exclude_patterns', [])
        
        for pattern in patterns:
            for file_path in base_path.rglob(pattern):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                should_exclude = False
                for exclude_pattern in exclude_patterns:
                    if file_path.match(exclude_pattern):
                        should_exclude = True
                        break
                
                if not should_exclude and file_path.is_file():
                    files.append(file_path)
        
        return files
    
    def process_framework(self, framework_name: str) -> List[ChunkInfo]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –æ–¥–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        framework_config = self.config['frameworks'][framework_name]
        parser = DocumentParser(framework_config)
        
        files = self.get_framework_files(framework_name)
        print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è {framework_name}: {len(files)}")
        
        all_chunks = []
        base_path = Path(framework_config['path'])
        
        for file_path in tqdm(files, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {framework_name}"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–∞–π–ª—ã
                if len(content.strip()) < 100:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                relative_path = file_path.relative_to(base_path)
                source_name = f"{framework_config['prefix']}/{relative_path}"
                
                # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
                chunks = parser.split_by_headers(content, str(source_name))
                all_chunks.extend(chunks)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}")
                continue
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è {framework_name}: {len(all_chunks)}")
        return all_chunks
    
    def add_chunks_to_db(self, chunks: List[ChunkInfo]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if not chunks:
            return
        
        current_count = self.collection.count()
        framework = chunks[0].framework
        
        print(f"üíæ –î–æ–±–∞–≤–ª—è–µ–º {len(chunks)} —á–∞–Ω–∫–æ–≤ {framework} –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
        
        for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ {framework}"):
            try:
                embedding = self.embedder.encode(chunk.full_text).tolist()
                
                # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
                chunk_id = f"{framework}-chunk-{current_count + i}"
                
                self.collection.add(
                    ids=[chunk_id],
                    documents=[chunk.full_text],
                    metadatas=[{
                        "source": chunk.source,
                        "heading": chunk.heading,
                        "framework": chunk.framework,
                        "type": chunk.metadata.get("type", "markdown"),
                        "size": chunk.metadata.get("size", 0)
                    }],
                    embeddings=[embedding]
                )
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —á–∞–Ω–∫–∞ {i}: {e}")
                continue
    
    def process_all_frameworks(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤–∫–ª—é—á–µ–Ω–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏"""
        self.init_database()
        self.init_embedder()
        
        initial_count = self.collection.count()
        
        for framework_name, framework_config in self.config['frameworks'].items():
            if framework_config.get('enabled', True):
                print(f"\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {framework_config['name']}...")
                chunks = self.process_framework(framework_name)
                self.add_chunks_to_db(chunks)
        
        final_count = self.collection.count()
        added_count = final_count - initial_count
        
        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {added_count}")
        print(f"üìö –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {final_count}")
    
    def rebuild_database(self):
        """–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—É—é –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
        db_config = self.config['database']
        try:
            self.client = chromadb.PersistentClient(path=db_config['path'])
            self.client.delete_collection(db_config['collection_name'])
            print("üóëÔ∏è –°—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞")
        except:
            pass
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
        self.collection = self.client.create_collection(db_config['collection_name'])
        print("üÜï –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏
        self.process_all_frameworks()
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        if not self.collection:
            self.init_database()
        
        total_docs = self.collection.count()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        sample_size = min(1000, total_docs)
        sample_results = self.collection.get(limit=sample_size)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º
        framework_counts = {}
        for metadata in sample_results["metadatas"]:
            framework = metadata.get("framework", "unknown")
            framework_counts[framework] = framework_counts.get(framework, 0) + 1
        
        # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –Ω–∞ –≤—Å—é –±–∞–∑—É
        estimated_counts = {}
        for fw, count in framework_counts.items():
            estimated_counts[fw] = int((count / sample_size) * total_docs)
        
        return {
            "total_documents": total_docs,
            "frameworks": estimated_counts,
            "sample_size": sample_size
        }

def main():
    parser = argparse.ArgumentParser(description="–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ RAG —Å–∏—Å—Ç–µ–º—ã")
    parser.add_argument("command", choices=["add", "rebuild", "stats"], 
                       help="–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    parser.add_argument("--config", default="config.yaml", 
                       help="–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É")
    parser.add_argument("--framework", 
                       help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫")
    
    args = parser.parse_args()
    
    manager = DocumentationManager(args.config)
    
    if args.command == "add":
        if args.framework:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            manager.init_database()
            manager.init_embedder()
            chunks = manager.process_framework(args.framework)
            manager.add_chunks_to_db(chunks)
        else:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤
            manager.process_all_frameworks()
    
    elif args.command == "rebuild":
        manager.rebuild_database()
    
    elif args.command == "stats":
        stats = manager.get_stats()
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö:")
        print(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
        print("–ü–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º:")
        for framework, count in stats['frameworks'].items():
            print(f"  {framework.upper()}: {count}")

if __name__ == "__main__":
    main()
